import streamlit as st
import os
from pathlib import Path
import tempfile # For creating temporary directories
import hashlib # For creating a hash of file contents for caching

# Import a reference to your rag_app module to access its functions and variables
import rag_app 
# Import LlamaIndex Settings to check and display configuration
from llama_index.core import Settings, VectorStoreIndex # Assuming VectorStoreIndex is returned by build_vector_index
# We might need Document and BaseNode if we are passing them around
from llama_index.core.schema import Document, BaseNode 
from llama_index.core.llms import LLM # For type hinting
from llama_index.core.query_engine import BaseQueryEngine # For type hinting

# Placeholder for functions that would come from rag_app.py or be redefined here
# In a real scenario, rag_app.py might be refactored into a library or its functions copied/adapted.

# Global settings (mimicking parts of rag_app.py's setup)
# These should ideally be loaded once.
# For Streamlit, we'll use its caching for expensive resources like models and indexes.

# --- Functions that would ideally be imported or adapted from rag_app.py ---

# Real model and settings initialization function
@st.cache_resource(show_spinner="æ­£åœ¨åˆå§‹åŒ–æ¨¡å‹å’Œè®¾ç½® (é¦–æ¬¡è¿è¡Œå¯èƒ½éœ€è¦ä¸€äº›æ—¶é—´)...")
def initialize_models_and_settings_real() -> bool:
    """
    Initializes embedding models, the default OpenAI LLM, and LlamaIndex Settings.
    Returns True if successful, False otherwise.
    """
    st.write("æ­£åœ¨å°è¯•åˆå§‹åŒ–åµŒå…¥æ¨¡å‹å’Œ LLM (OpenAI gpt-4o)...")
    initialization_success = False
    
    try:
        rag_app.initialize_embed_model()
        embed_model_name = Settings.embed_model.model_name if hasattr(Settings.embed_model, 'model_name') else Settings.embed_model.__class__.__name__
        st.write(f"åµŒå…¥æ¨¡å‹å·²åˆå§‹åŒ–: {embed_model_name}")
        
        # Initialize the default OpenAI LLM and set it in Settings
        configured_llm = rag_app.initialize_llms() 

        if Settings.llm and hasattr(Settings.llm, 'model'):
            st.write(f"é»˜è®¤ LLM å·²åˆå§‹åŒ–: {Settings.llm.model}")
            st.success("æ¨¡å‹å’Œ LlamaIndex è®¾ç½®å·²å°±ç»ªï¼")
            initialization_success = True
        else:
            st.error("é»˜è®¤ LLM (OpenAI) æœªèƒ½åˆå§‹åŒ–ã€‚è¯·æ£€æŸ¥ API å¯†é’¥å’Œæ§åˆ¶å°æ—¥å¿—ã€‚")
            initialization_success = False
            
    except Exception as e:
        st.error(f"æ¨¡å‹åˆå§‹åŒ–æœŸé—´å‘ç”Ÿé”™è¯¯: {e}")
        st.warning("è¯·æ£€æŸ¥æ§åˆ¶å°ä»¥è·å–æ¥è‡ª rag_app.py çš„æ›´è¯¦ç»†é”™è¯¯ä¿¡æ¯ã€‚")
        initialization_success = False
    
    return initialization_success

# Function to create a hash from a list of UploadedFile objects
# This helps in making the cache specific to the set of uploaded files.
def create_files_hash(uploaded_files):
    hasher = hashlib.md5()
    for uploaded_file in sorted(uploaded_files, key=lambda f: f.name):
        uploaded_file.seek(0)
        hasher.update(uploaded_file.getvalue())
    return hasher.hexdigest()

# Refactored function to build the index, LlamaParse logic removed
@st.cache_resource(show_spinner="æ­£åœ¨å¤„ç†æ–‡æ¡£å¹¶æ„å»ºç´¢å¼•...")
def build_index_from_uploaded_files(_files_tuple, chunk_size: int, chunk_overlap: int) -> VectorStoreIndex | None:
    """Processes uploaded files using LlamaParse if available, otherwise basic loaders, builds, and returns a VectorStoreIndex."""
    _uploaded_files = list(_files_tuple)

    if not _uploaded_files:
        st.warning("æ²¡æœ‰ä¸Šä¼ æ–‡ä»¶éœ€è¦å¤„ç†ã€‚") 
        return None

    all_loaded_documents = []
    with tempfile.TemporaryDirectory() as temp_dir_path_str:
        temp_dir = Path(temp_dir_path_str)
        st.write(f"å·²åˆ›å»ºç”¨äºå¤„ç†çš„ä¸´æ—¶ç›®å½•: {temp_dir}")
        
        saved_file_paths = []
        for uploaded_file in _uploaded_files:
            file_path = temp_dir / uploaded_file.name
            try:
                with open(file_path, "wb") as f:
                    uploaded_file.seek(0) # Ensure we read from the beginning
                    f.write(uploaded_file.getbuffer())
                saved_file_paths.append(str(file_path))
            except Exception as e:
                st.error(f"ä¿å­˜ä¸Šä¼ æ–‡ä»¶ '{uploaded_file.name}' åˆ°ä¸´æ—¶ç›®å½•æ—¶å‡ºé”™: {e}")
                continue 
        
        if not saved_file_paths:
            st.error("æœªèƒ½ä¿å­˜ä»»ä½•ä¸Šä¼ çš„æ–‡ä»¶ä»¥ä¾›å¤„ç†ã€‚")
            return None

        # --- LlamaParse First, Standard Loading Fallback ---
        llama_cloud_api_key_is_set = bool(rag_app.LLAMA_CLOUD_API_KEY) # Check if key is loaded in rag_app

        if llama_cloud_api_key_is_set:
            st.write(f"æ£€æµ‹åˆ° LLAMA_CLOUD_API_KEYã€‚å°†å°è¯•ä½¿ç”¨ LlamaParse è§£ææ–‡ä»¶: {saved_file_paths}")
            try:
                # Assuming parse_documents_with_llamaparse is now in rag_app
                parsed_docs = rag_app.parse_documents_with_llamaparse(file_paths=saved_file_paths, result_type="text")
                if parsed_docs:
                    all_loaded_documents.extend(parsed_docs)
                    st.write(f"LlamaParse æˆåŠŸè§£æäº† {len(parsed_docs)} ä¸ªæ–‡æ¡£ã€‚")
                else:
                    st.write("LlamaParse æœªèƒ½ä»æ–‡ä»¶ä¸­è§£æå‡ºä»»ä½•æ–‡æ¡£ï¼Œæˆ–è€…è¿”å›äº†ç©ºç»“æœã€‚")
            except Exception as e:
                st.warning(f"LlamaParse å¤„ç†æœŸé—´å‡ºé”™: {e}ã€‚å°†å°è¯•ä½¿ç”¨æ ‡å‡†åŠ è½½å™¨å›é€€ã€‚")
        
        # Fallback or if LlamaParse is not configured/failed for some files
        # We should process files that LlamaParse might not have (fully) processed or if it wasn't used.
        # If LlamaParse was attempted and returned some documents, we might not want to re-process them with load_documents.
        # For simplicity now, if LlamaParse didn't populate all_loaded_documents, load_documents will try all.
        # A more sophisticated approach would be to track which files LlamaParse failed on.
        
        if not all_loaded_documents: # If LlamaParse wasn't used or failed to produce any docs
            if llama_cloud_api_key_is_set:
                 st.write(f"LlamaParse æœªè¿”å›ä»»ä½•æ–‡æ¡£ï¼Œæ­£åœ¨ä½¿ç”¨æ ‡å‡†åŠ è½½å™¨å›é€€å¤„ç†: {saved_file_paths}")
            else:
                 st.write(f"LLAMA_CLOUD_API_KEY æœªè®¾ç½®ã€‚æ­£åœ¨ä½¿ç”¨æ ‡å‡†åŠ è½½å™¨å¤„ç†: {saved_file_paths}")
            
            try:
                # Use load_documents for all saved file paths
                loaded_docs_fallback = rag_app.load_documents(saved_file_paths)
                all_loaded_documents.extend(loaded_docs_fallback)
                st.write(f"æ ‡å‡†åŠ è½½å™¨åŠ è½½äº† {len(loaded_docs_fallback)} ä¸ªæ–‡æ¡£ã€‚")
            except Exception as e:
                st.error(f"ä½¿ç”¨æ ‡å‡†åŠ è½½å™¨åŠ è½½æ–‡æ¡£æ—¶å‡ºé”™: {e}")
        
        if not all_loaded_documents:
            st.error("æœªèƒ½æˆåŠŸåŠ è½½æˆ–è§£æä»»ä½•æ–‡æ¡£ã€‚æ— æ³•æ„å»ºç´¢å¼•ã€‚")
            return None

        st.write(f"è¦è¿›è¡Œåˆ†å—çš„æ€»æ–‡æ¡£æ•°: {len(all_loaded_documents)}")
        try:
            # Pass the chunk_size and chunk_overlap to chunk_documents
            nodes = rag_app.chunk_documents(all_loaded_documents, chunk_size=chunk_size, chunk_overlap=chunk_overlap)
            if not nodes:
                st.error("åˆ†å—æœªäº§ç”Ÿä»»ä½•èŠ‚ç‚¹ã€‚æ— æ³•æ„å»ºç´¢å¼•ã€‚")
                return None
            st.write(f"åˆ†å—å®Œæˆã€‚ç”Ÿæˆäº† {len(nodes)} ä¸ªèŠ‚ç‚¹ã€‚")
        except Exception as e:
            st.error(f"æ–‡æ¡£åˆ†å—æœŸé—´å‡ºé”™: {e}")
            return None

        try:
            vector_index = rag_app.build_vector_index(nodes)
            if not vector_index:
                st.error("æ„å»ºå‘é‡ç´¢å¼•å¤±è´¥ã€‚")
                return None
            st.success("å‘é‡ç´¢å¼•æ„å»ºæˆåŠŸï¼")
            return vector_index 
        except Exception as e:
            st.error(f"æ„å»ºå‘é‡ç´¢å¼•æ—¶å‡ºé”™: {e}")
            return None

# Function for getting response (remains largely the same)
@st.cache_data(show_spinner="æ­£åœ¨æŸ¥è¯¢...") 
def get_rag_response_real(_query_engine: BaseQueryEngine | None, query_text: str):
    # (Keep the implementation of get_rag_response_real as it was)
    if not query_text:
        return "è¯·è¾“å…¥æŸ¥è¯¢å†…å®¹ã€‚"
    if not _query_engine:
        st.error("æŸ¥è¯¢å¼•æ“ä¸å¯ç”¨ã€‚")
        return "é”™è¯¯ï¼šæŸ¥è¯¢å¼•æ“æœªå°±ç»ªã€‚"
    
    st.write(f"æ­£åœ¨æŸ¥è¯¢: '{query_text}'")
    try:
        response = _query_engine.query(query_text)
        return str(response)
    except Exception as e:
        st.error(f"æŸ¥è¯¢æ‰§è¡ŒæœŸé—´å‡ºé”™: {e}")
        # Check for specific error types if needed
        if "Connection error" in str(e):
             st.error("æ— æ³•è¿æ¥åˆ° LLM APIã€‚è¯·æ£€æŸ¥ç½‘ç»œè¿æ¥å’Œ API å¯†é’¥ã€‚")
        return f"é”™è¯¯ï¼šæ— æ³•è·å–å“åº”ã€‚ {e}"

# --- Streamlit App UI ---
st.set_page_config(page_title="æˆ‘çš„ RAG åº”ç”¨", layout="wide")
st.title("ğŸ“š æˆ‘çš„å®šåˆ¶ RAG åº”ç”¨")
st.write("ä¸Šä¼ æ‚¨çš„æ–‡æ¡£ï¼Œæå‡ºé—®é¢˜ï¼Œè·å–ç”± RAG é©±åŠ¨çš„ç­”æ¡ˆï¼")

models_initialized = initialize_models_and_settings_real()

# Use session state to store the vector index and query engine
if 'vector_index' not in st.session_state:
    st.session_state.vector_index = None 
if 'query_engine' not in st.session_state:
    st.session_state.query_engine = None 

with st.sidebar:
    st.header("âš™ï¸ æ–‡æ¡£å¤„ç†") # CN: Simplified header
    
    # Removed LlamaParse Checkbox
    # use_llamaparse = st.checkbox("âœ¨ ä½¿ç”¨ LlamaParse ... ", value=True) 
    
    # File Uploader
    uploaded_files = st.file_uploader(
        "é€‰æ‹©æ–‡ä»¶ (.txt, .pdf, .md, .docx)", 
        accept_multiple_files=True,
        type=["txt", "pdf", "md", "docx"]
    )

    # Chunking parameters
    st.subheader("åˆ†å—å‚æ•°") # CN: Chunking Parameters
    chunk_size_input = st.number_input("å—å¤§å° (Chunk Size)", min_value=100, max_value=8000, value=1000, step=100, help="æ¯ä¸ªæ–‡æœ¬å—çš„ç›®æ ‡å¤§å°ï¼ˆä»¥ Token è®¡ï¼‰ã€‚")
    chunk_overlap_input = st.number_input("å—é‡å  (Chunk Overlap)", min_value=0, max_value=1000, value=200, step=50, help="ç›¸é‚»æ–‡æœ¬å—ä¹‹é—´çš„é‡å  Token æ•°ã€‚")

    # Process Button Logic
    if uploaded_files:
        st.write(f"å·²é€‰æ‹© {len(uploaded_files)} ä¸ªæ–‡ä»¶:")
        for f in uploaded_files:
            st.caption(f.name)
            
        if st.button("âš™ï¸ å¤„ç†æ–‡æ¡£å¹¶æ„å»ºç´¢å¼•"):
            if models_initialized:
                # Pass tuple for caching
                files_tuple = tuple(uploaded_files) # Convert to tuple for st.cache_resource
                
                # Call the refactored function to build the index
                # The 'use_llamaparse' argument is no longer needed as logic is internal to build_index_from_uploaded_files
                # Pass the chunk_size and chunk_overlap from the UI
                vector_index_result = build_index_from_uploaded_files(files_tuple, chunk_size=chunk_size_input, chunk_overlap=chunk_overlap_input) 
                
                if vector_index_result:
                    st.session_state.vector_index = vector_index_result
                    st.success("æ–‡æ¡£å¤„ç†å’Œç´¢å¼•æ„å»ºå®Œæˆï¼")
                    # Now, create the query engine using the built index and default LLM
                    with st.spinner("æ­£åœ¨åˆ›å»ºæŸ¥è¯¢å¼•æ“..."): # CN
                        try:
                            # create_query_engine now defaults to Settings.llm (gpt-4o)
                            query_engine = rag_app.create_query_engine(st.session_state.vector_index)
                            if query_engine:
                                st.session_state.query_engine = query_engine
                                st.success("æŸ¥è¯¢å¼•æ“å·²å°±ç»ªï¼") # CN
                            else:
                                st.error("æœªèƒ½åˆ›å»ºæŸ¥è¯¢å¼•æ“ã€‚") # CN 
                                st.session_state.query_engine = None # Ensure it's cleared
                        except Exception as e:
                             st.error(f"åˆ›å»ºæŸ¥è¯¢å¼•æ“æ—¶å‡ºé”™: {e}") # CN
                             st.session_state.query_engine = None
                else:
                    # If index building failed, clear relevant states
                    st.session_state.vector_index = None
                    st.session_state.query_engine = None
                    st.error("æœªèƒ½å¤„ç†æ–‡æ¡£å¹¶æ„å»ºç´¢å¼•ã€‚è¯·æŸ¥çœ‹ä¸Šæ–¹é”™è¯¯ä¿¡æ¯ã€‚") # CN
            else:
                st.error("æ¨¡å‹æœªåˆå§‹åŒ–ã€‚æ— æ³•å¤„ç†æ–‡æ¡£ã€‚")
    else:
        st.write("è¯·ä¸Šä¼ ä¸€äº›æ–‡æ¡£ä»¥å¼€å§‹ã€‚")

st.header("ğŸ’¬ æé—®")

# Check if query engine is ready before allowing queries
if st.session_state.query_engine:
    st.info("æŸ¥è¯¢å¼•æ“å·²å‡†å¤‡å°±ç»ªï¼Œå¯ä»¥å¼€å§‹æé—®ã€‚") # CN
    query_text = st.text_input("åœ¨æ­¤è¾“å…¥æ‚¨çš„é—®é¢˜:", key="query_input")
    if st.button("æäº¤æŸ¥è¯¢", key="submit_button"):
        if query_text:
            # Directly use the query engine stored in session state
            response_text = get_rag_response_real(st.session_state.query_engine, query_text)
            st.subheader("å›ç­”:")
            st.markdown(response_text)
            # TODO: Optionally display source nodes from response object if needed
        else:
            st.warning("è¯·è¾“å…¥é—®é¢˜ã€‚")
elif uploaded_files and not st.session_state.vector_index: # Files uploaded but not processed
     st.warning("è¯·å…ˆç‚¹å‡»ä¾§è¾¹æ æŒ‰é’®å¤„ç†ä¸Šä¼ çš„æ–‡æ¡£ã€‚") # CN
elif not uploaded_files:
     st.info("è¯·ä½¿ç”¨ä¾§è¾¹æ ä¸Šä¼ æ–‡æ¡£ä»¥å¯ç”¨æŸ¥è¯¢ã€‚")
else: # Index might be built, but query engine creation failed
     st.error("æŸ¥è¯¢å¼•æ“æœªèƒ½åˆ›å»ºï¼Œæ— æ³•è¿›è¡ŒæŸ¥è¯¢ã€‚è¯·æ£€æŸ¥å¤„ç†æ­¥éª¤ä¸­çš„é”™è¯¯ã€‚") # CN

# For running this app: streamlit run streamlit_app.py 